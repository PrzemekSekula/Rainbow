{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning with fixed Q-targets\n",
    "---\n",
    "In the previous notebook we were using the same neural network to estimate $Q(s, a, w)$ and $Q(s' a, w)$ (required for $TD_{target}$ estimates). As a result we were updating a guess with a guess. This can potentially lead to instability due to occurrence of some harmful correlations. To avoid this, we can use another, fix set of weights $w^-$ that is not changed during the learnig step to generate $TD_{target}$ according to the formula: $TD_{target} = R + \\gamma \\max\\limits_{a'} Q(S_{t+1}, a', w^-)$ \n",
    "In practice we create a copy of the base neural network (so called `local neural network`). This copy is called a `target neural network`. Then, we are using the target neural network to estimate $TD_{target}$ and update the local neural network only.\n",
    "\n",
    "However, we still want the target neural network to be as close to the local neural network as possible, so we need to update it somehow. There are two main approaches:\n",
    "- update target neural network every $n$ steps or every $n$ episodes - this approach is used in this solution\n",
    "- perform a soft-update of the target weights according to the formula: $w_{target} = \\tau*w_{local} + (1 - \\tau)*w_{target}$. Where $\\tau$ is a parameter that determines update speed (usually very small, like 0.001).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config.ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow.fixed_qtarget_agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=env.observation_space.shape[0], \n",
    "    action_size=env.action_space.n, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=config.MAX_EPISODES, \n",
    "        max_t=config.MAX_TIMESTEPS, \n",
    "        eps_start=config.EPSILON_START, \n",
    "        eps_end=config.EPSILON_END, \n",
    "        eps_decay=config.EPSILON_DECAY,\n",
    "        expected_reward = config.EXPECTED_REWARD,\n",
    "        update_target_every = 1\n",
    "):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args:\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "        upate_target_every (int): how often should the target \n",
    "            network be updated. Default: 1 (per every episode) \n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode\n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "                \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.target_hard_update()\n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}', end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
