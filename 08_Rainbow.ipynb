{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow\n",
    "---\n",
    "It this notebook, the noise neural network is used. There are two main aspects of this implementation.\n",
    "\n",
    "1) The layers in advantage and value blocks have factorized Gaussian nosie added to them while in training mode. A layer with the noise is implemented in the `NoisyLinear` class. In the \"classical\" layers, the output (with no activation function) is computed as:\n",
    "$$y=w*x+b$$\n",
    "in the noisy layers it is:\n",
    "$$y=(\\mu^w + \\sigma^w \\odot \\epsilon^w)x + \\mu^b + \\sigma^b \\odot \\epsilon^b$$\n",
    "\n",
    "where $\\mu^w \\in \\mathbb{R}^{q \\times p}$, $\\sigma^w \\in \\mathbb{R}^{q \\times p}$, $\\mu^b \\in \\mathbb{R}^q$, and $\\sigma^b \\in \\mathbb{R}^q$ are learnable parameters and $\\epsilon^w \\in \\mathbb{R}^{q \\times p}$, $\\epsilon^b \\in \\mathbb{R}^q$ are noise random variables. In this notation $p$ and $q$ correspond to the number of layer inputs and outputs and $\\odot$ is an elementwise multiplication.\n",
    "\n",
    "\n",
    "The noise can be generated with one of the following approaches:\n",
    "- **Independent Gaussian noise** -  appied independently to each weight and bias\n",
    "- **Factorized Gaussian noise** - produces two random noise vectors $\\epsilon_{in} \\in \\mathbb{R}^p$ and $\\epsilon_{out} \\in \\mathbb{R}^q$. Then we use a function $f(x) = sgn(x) \\sqrt{|x|}$ to compute $f(\\epsilon_{in})$ and $f(\\epsilon_{out})$. Finally we set \n",
    "$$\\epsilon^w = f(\\epsilon_{in}) \\otimes f(\\epsilon_{out})$$\n",
    "$$\\epsilon^b = f(\\epsilon_{out})$$\n",
    "where $\\otimes$ is a generalized outer product.\n",
    "\n",
    "2) Instead of doing e-greedy exploration we can use the noise to explore. This way we are always trying to choose the best action, and the exploration (choosing non-optimal actions) comes from the noise itself. In my code you can choose between noise-based and e-greedy exploration. It is controlled by the `explore_with_noise` parameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config.ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow.rainbow_agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=env.observation_space.shape[0], \n",
    "    action_size=env.action_space.n,\n",
    "    buffer_size = int(1e5),\n",
    "    batch_size = 64,\n",
    "    gamma = 0.99,\n",
    "    lr = 5e-4,\n",
    "    update_every = 4, # How often to update the network\n",
    "    device=device,\n",
    "    # PER parameters\n",
    "    per_alpha = 0.2,\n",
    "    per_beta_start = 0.4,\n",
    "    per_beta_frames = 1e5,\n",
    "    per_prior_eps = 1e-6, \n",
    "    # Dueling parameters\n",
    "    clip_grad=10, \n",
    "    #N-step parameters\n",
    "    n_steps = 3, \n",
    "    # Distributional parameters\n",
    "    atom_size=10, # Originally it was 51\n",
    "    v_min=0,\n",
    "    v_max=200,\n",
    "    # NoisyNet parameters\n",
    "    explore_with_noise=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=config.MAX_EPISODES, \n",
    "        max_t=config.MAX_TIMESTEPS, \n",
    "        eps_start=config.EPSILON_START, \n",
    "        eps_end=config.EPSILON_END, \n",
    "        eps_decay=config.EPSILON_DECAY,\n",
    "        expected_reward = config.EXPECTED_REWARD,\n",
    "        update_target_every = 4\n",
    "): \n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args:\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "        upate_target_every (int): how often should the target \n",
    "            network be updated. Default: 1 (per every episode) \n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode\n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "                \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.target_hard_update()\n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}', end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
