{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Double Deep Q-Learning \n",
    "---\n",
    "In the previous notebook we were using additional neural network `target neural network` to estimate $TD_{target}$ according to the formula: \n",
    "$$TD_{target} = R + \\gamma \\max\\limits_{a'} Q(S_{t+1}, \\argmax\\limits_{a'}Q(S_{t+1}, a', w^-), w^-)$$ \n",
    "In other words, we were using target neural network to select the best action $a'$, and then we were using the same network to estimate the value of this action. Such an approach can be prone to overestimation. \n",
    "To solve this problem, we may select the best action using other set of weights, and then estimate the value of this action with target neural network. In fact, we already have additonal set of weights $w$ that comes from the `local neural network`, so our $TD_{target}$ estimations can be performed as follows:\n",
    "$$TD_{target} = R + \\gamma \\max\\limits_{a'} Q(S_{t+1}, \\argmax\\limits_{a'}Q(S_{t+1}, a', w^-), w)$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config.ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow.double_dql_agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=env.observation_space.shape[0], \n",
    "    action_size=env.action_space.n,\n",
    "    device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=config.MAX_EPISODES, \n",
    "        max_t=config.MAX_TIMESTEPS, \n",
    "        eps_start=config.EPSILON_START, \n",
    "        eps_end=config.EPSILON_END, \n",
    "        eps_decay=config.EPSILON_DECAY,\n",
    "        expected_reward = config.EXPECTED_REWARD,\n",
    "        update_target_every = 4\n",
    "):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args:\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "        upate_target_every (int): how often should the target \n",
    "            network be updated. Default: 1 (per every episode) \n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode\n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "                \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.target_hard_update()\n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}', end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
