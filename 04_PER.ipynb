{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prioritized Experience Replay\n",
    "---\n",
    "In this notebook I am using prioritized experience replay. The general idea is to learn mainly from experiences that we can learn a lot from. Assumption: If there is a large $TD_{error}$ (defined as $\\delta_t = R + \\gamma \\max\\limits_{a'} Q(S', \\argmax\\limits_{a'}Q(S', a', w^-), w) - q(S, a, w)$) it means, that we can learn a lot from this experience, so we sample it more often. The sample chance is proportional to the latest $TD_{error}$. The probability of selecting each sample follows the formula: $P(i) = \\frac{p_i}{\\sum_k{p_k}}$, where $p_t=|\\delta_t|$\n",
    "\n",
    "As the sampling is not uniform any more, we have to modify the update rule as well. The way we sample the experiences should match the underlying distribution they came from. In uniform sampling it is preserved by default, but with non-uniform sampling we need to introduce the sampling weights $(\\frac{1}{N}*\\frac{1}{P(i)})$ where $N$ is the size of the replay buffer. As a result, our update rule looks like this: $\\Delta w = \\alpha (\\frac{1}{N}*\\frac{1}{P(i)}) \\delta_t \\nabla_w Q(S, a, w)$\n",
    "\n",
    "There are couple of modifications that make this idea even more useful:\n",
    "1) Experiences with $TD_{error}=0$ will never be sampled, and want every experience to have a chance of being used. To ensure it we add a small positive value $\\epsilon$ to each probability ($p_t=|\\delta_t| + \\epsilon$). In the code, this value is denoted as `per_prior_eps`.\n",
    "2) We do want to control how often the experiences with high $TD_{error}$ are being used. We are doing it, but introducing additional parameter $\\alpha$ (`per_alpha` in code) and then sampling with a probability $P(i) = \\frac{p_i^\\alpha}{\\sum_k{p_k^\\alpha}}$. If $\\alpha$ is equal to 0, all experiences are sampled uniformly. If $\\alpha$ is eual to 1, we are fully leveraging the prioritized experience. \n",
    "3) We introduce additional parameter $\\beta$ to control how much we want to use weight sampling in the update rule. The final update rule looks like: $\\Delta w = \\alpha (\\frac{1}{N}*\\frac{1}{P(i)})^\\beta \\delta_t \\nabla_w Q(S, a, w)$. Usually we set initial $\\beta$ to a low value, and increase it in time up to 1. In code, it is implemented by using `per_beta_start` (initial $\\beta$) and `per_beta_frames` (number of steps necessary to push $\\beta$ to 1). $\\beta$ values are being increased between `per_beta_start` and 1 linearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config.ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow.per_agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=env.observation_space.shape[0], \n",
    "    action_size=env.action_space.n,\n",
    "    buffer_size = int(1e5),\n",
    "    batch_size = 64,\n",
    "    gamma = 0.99,\n",
    "    lr = 5e-4,\n",
    "    update_every = 4, # How often to update the network\n",
    "    device=device,\n",
    "    # PER parameters\n",
    "    per_alpha = 0.2,\n",
    "    per_beta_start = 0.4,\n",
    "    per_beta_frames = 1e5,\n",
    "    per_prior_eps = 1e-6,    \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=config.MAX_EPISODES, \n",
    "        max_t=config.MAX_TIMESTEPS, \n",
    "        eps_start=config.EPSILON_START, \n",
    "        eps_end=config.EPSILON_END, \n",
    "        eps_decay=config.EPSILON_DECAY,\n",
    "        expected_reward = config.EXPECTED_REWARD,\n",
    "        update_target_every = 4\n",
    "):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args:\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "        upate_target_every (int): how often should the target \n",
    "            network be updated. Default: 1 (per every episode) \n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode\n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "                \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.target_hard_update()\n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}', end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
